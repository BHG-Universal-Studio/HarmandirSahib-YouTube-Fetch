#!/usr/bin/env python3

import requests

import xml.etree.ElementTree as ET

from datetime import datetime, timezone

import firebase_admin

from firebase_admin import credentials, firestore

import json

import os

import sys

import time

import re



# ---------------- CONFIG ----------------

CHANNEL_IDS = [

    "UC884UDwNldmpdEiS1mgtijA",
    "UC_JnnWTC6gHc59JwfMPTjdw",
    "UCQroafhIKCxeQ0e9jj-O51Q",
    "UC71aJD7c8-FWf-nJ7ug2sfg",
    "UCUjIneSnBylQOqAk7n7i33A",
    "UC1wecYlMxn33DPHrhHHUyVw",
    "UCh0LDn5Drt44tITPoQiiJ6Q",
    "UCBe8nwY2SqWlrGKKcmxB0_w",
    
]



COLLECTION_NAME = "Shorts"

ALL_IDS_DOC = "-All_Shorts_Videos_Ids"

MAX_DURATION_SECONDS = 80




SERVICE_ACCOUNT_JSON = os.environ.get("FIREBASE_SERVICE_ACCOUNT")

YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY")



if not SERVICE_ACCOUNT_JSON:

    print("âŒ FIREBASE_SERVICE_ACCOUNT env var missing")

    sys.exit(1)



if not YOUTUBE_API_KEY:

    print("âŒ YOUTUBE_API_KEY env var missing")

    sys.exit(1)



NS = {

    "atom": "http://www.w3.org/2005/Atom",

    "yt": "http://www.youtube.com/xml/schemas/2015"

}



# ---------------- FIREBASE INIT ----------------

if not firebase_admin._apps:

    cred = credentials.Certificate(json.loads(SERVICE_ACCOUNT_JSON))

    firebase_admin.initialize_app(cred)



db = firestore.client()



# ---------------- READ EXISTING IDS (1 READ) ----------------

ids_doc_ref = db.collection(COLLECTION_NAME).document(ALL_IDS_DOC)

ids_doc = ids_doc_ref.get()



existing_ids = set()

if ids_doc.exists:

    existing_ids = set(ids_doc.to_dict().get("video_id", []))



print(f"ğŸ“¦ Existing video IDs in Firebase: {len(existing_ids)}")



# ---------------- COUNTERS ----------------

total_fetched = 0

total_skipped_existing = 0

total_skipped_live = 0

total_skipped_short = 0

total_inserted = 0

new_ids_added = []



# ---------------- RSS FETCH ----------------

def fetch_videos_from_channel(channel_id):

    url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"

    try:

        response = requests.get(url, timeout=20)

        response.raise_for_status()

    except Exception as e:

        print(f"âš ï¸ Error fetching channel {channel_id}: {e}")

        return []



    root = ET.fromstring(response.text)

    videos = []



    entries = root.findall("atom:entry", NS)

    

    for entry in entries:

        title_el = entry.find("atom:title", NS)

        video_id_el = entry.find("yt:videoId", NS)

        published_el = entry.find("atom:published", NS)



        if title_el is None or video_id_el is None or published_el is None:

            continue



        published_dt = datetime.fromisoformat(

            published_el.text.replace("Z", "+00:00")

        ).astimezone(timezone.utc)



        video_id = video_id_el.text.strip()



        videos.append({

            "video_id": video_id,

            "title": title_el.text.strip(),

            "url": f"https://www.youtube.com/watch?v={video_id}",

            "imageUrl": f"https://i.ytimg.com/vi/{video_id}/maxresdefault.jpg",

            "published": published_dt

        })



    return videos



# ---------------- HELPER: CHUNK LIST ----------------

def chunk_list(data, chunk_size):

    """Yield successive chunks from list."""

    for i in range(0, len(data), chunk_size):

        yield data[i:i + chunk_size]



# ---------------- API HELPER: CHECK LIVE STATUS ----------------

def get_live_status_batch(video_ids):

    """

    Checks 'snippet.liveBroadcastContent'.

    Returns a set of video_ids that ARE live or upcoming (to be excluded).

    """

    live_or_upcoming_ids = set()

    

    # Process in chunks of 50 (YouTube API limit is 50)

    # You requested 30, but 50 is safe. We can use 30 to be extra safe.

    CHUNK_SIZE = 30 

    

    for chunk in chunk_list(video_ids, CHUNK_SIZE):

        url = "https://www.googleapis.com/youtube/v3/videos"

        params = {

            "part": "snippet",

            "id": ",".join(chunk),

            "key": YOUTUBE_API_KEY,

            "maxResults": 50

        }

        

        try:

            r = requests.get(url, params=params, timeout=15)

            r.raise_for_status()

            data = r.json()

            

            for item in data.get("items", []):

                vid = item["id"]

                # 'none' = completed/vod (keep)

                # 'live' = currently live (exclude)

                # 'upcoming' = scheduled (exclude)

                broadcast_content = item["snippet"].get("liveBroadcastContent", "none")

                

                if broadcast_content in ["live", "upcoming"]:

                    live_or_upcoming_ids.add(vid)

                    print(f"ğŸš« Detected Live/Upcoming stream: {vid} ({broadcast_content})")

                    

        except Exception as e:

            print(f"âš ï¸ Error checking live status: {e}")

    

    return live_or_upcoming_ids



# ---------------- API HELPER: FETCH DURATIONS ----------------

def iso8601_to_seconds(duration):

    match = re.match(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?", duration)

    if not match:

        return 0

    h = int(match.group(1) or 0)

    m = int(match.group(2) or 0)

    s = int(match.group(3) or 0)

    return h * 3600 + m * 60 + s



def fetch_durations_batch(video_ids):

    """

    Fetch durations for videos in chunks.

    """

    duration_map = {}

    CHUNK_SIZE = 50 



    for chunk in chunk_list(video_ids, CHUNK_SIZE):

        url = "https://www.googleapis.com/youtube/v3/videos"

        params = {

            "part": "contentDetails",

            "id": ",".join(chunk),

            "key": YOUTUBE_API_KEY,

            "maxResults": 50

        }



        try:

            r = requests.get(url, params=params, timeout=15)

            r.raise_for_status()

            data = r.json()



            for item in data.get("items", []):

                vid = item["id"]

                iso = item["contentDetails"]["duration"]

                duration_map[vid] = iso8601_to_seconds(iso)

        except Exception as e:

            print(f"âš ï¸ Error fetching durations: {e}")



    return duration_map



# ---------------- MAIN LOGIC ----------------

rss_videos = []



# 1. Gather all videos from RSS

for channel_id in CHANNEL_IDS:

    print(f"\nğŸ” Fetching channel: {channel_id}")

    videos = fetch_videos_from_channel(channel_id)

    print(f"ğŸ“º Videos in RSS: {len(videos)}")

    total_fetched += len(videos)

    rss_videos.extend(videos)



# 2. Filter out Existing IDs (Local Check)

candidates = []

for v in rss_videos:

    if v["video_id"] in existing_ids:

        total_skipped_existing += 1

        continue

    # De-duplicate duplicates within RSS feeds themselves

    if any(c["video_id"] == v["video_id"] for c in candidates):

        continue

    candidates.append(v)



print(f"\nğŸ“ Candidates after DB check: {len(candidates)}")



if not candidates:

    print("âœ… No new videos to process.")

    sys.exit(0)



candidate_ids = [v["video_id"] for v in candidates]



# 3. Check Live Status (API Call 1 - Batched)

print("\nğŸ“¡ Checking Live/Upcoming status...")

live_ids_to_exclude = get_live_status_batch(candidate_ids)

total_skipped_live = len(live_ids_to_exclude)



# Remove live videos from candidates

vod_candidates = [v for v in candidates if v["video_id"] not in live_ids_to_exclude]

vod_candidate_ids = [v["video_id"] for v in vod_candidates]



print(f"ğŸ“‰ Remaining after Live filter: {len(vod_candidates)}")



# 4. Check Durations (API Call 2 - Batched)

# Only check duration for videos that passed the Live check

print("\nâ±ï¸ Checking Durations...")

duration_map = fetch_durations_batch(vod_candidate_ids)



# 5. Insert Final Videos

print("\nğŸš€ Starting Firebase Insertion...")

for v in vod_candidates:
    vid = v["video_id"]
    duration = duration_map.get(vid, 0)

    # Duration Check (Shorts only)
    if duration >= MAX_DURATION_SECONDS:
        print(f"â­ï¸ Skipped long ({duration}s): {vid}")
        total_skipped_short += 1
        continue

    # Insert to Firebase
    db.collection(COLLECTION_NAME).document().set({
        "title": v["title"],
        "url": v["url"],
        "imageUrl": v["imageUrl"],
        "timestamp": int(v["published"].timestamp() * 1000),
        "video_id": vid,
    })

    existing_ids.add(vid)
    new_ids_added.append(vid)
    total_inserted += 1

    print(f"â• Inserted ({duration}s): {vid} - {v['title'][:30]}...")
    time.sleep(0.03)




# ---------------- UPDATE ID INDEX ----------------

if new_ids_added:

    print(f"\nğŸ’¾ Updating {ALL_IDS_DOC} index...")

    ids_doc_ref.set({

        "video_id": list(existing_ids),

        "ids_Count": len(existing_ids)

    }, merge=True)



# ---------------- SUMMARY ----------------

print("\n================ SUMMARY ================")

print(f"ğŸ“¥ Total RSS Fetched   : {total_fetched}")

print(f"â­ï¸  Skipped (Existing)  : {total_skipped_existing}")

print(f"ğŸš« Skipped (Live/Upc)  : {total_skipped_live}")

print(f"â±ï¸ Skipped long (â‰¥{MAX_DURATION_SECONDS}s) : {total_skipped_short}")

print(f"â• Videos Inserted     : {total_inserted}")

print(f"ğŸ“Š New Firebase Total  : {len(existing_ids)}")

print("========================================")
